{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph exploration & analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Objective :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the virality of memes/hashtags in a social network (Twitter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Context and motivations :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virality is, in social networks, an important issue for corporations, political campaigns and influencers as they spend enormous resources and efforts to make their products or messages go viral in order to catch attention and spread their influence/activities to a wider audience.\n",
    "Thus, understanding the complex mechanism of virality may help one control its effects over the network:\n",
    "- How does the network structure affect the diffusion? \n",
    "- How to model the contagion, etc.\n",
    "\n",
    "Proposal brought by the paper : the  broad idea is that network communities allow predict virality by its early spreading pattern. A simple, popular approach in studying hashtags diffusion is to consider hashtags as diseases and apply epidemic models. However, recent studies demonstrate that diseases and behaviors spread differently.\n",
    "We can see huge potentiality for applications in social media marketing : social networks could give better advice to their users as to which posts are likely to give best advertising Return on Investment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from itertools import product\n",
    "from itertools import permutations, combinations\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** CITATION ****\n",
    "Please cite our paper as follows, when you are using our dataset:\n",
    "Lilian Weng, Filippo Menczer, and Yong-Yeol Ahn. Virality Prediction and Community Structure in Social Networks. Nature Scientific Report. (3)2522, 2013.\n",
    "\n",
    "\n",
    "\n",
    "**** DATA SOURCE ****\n",
    "Sampled public tweets from Twitter streaming API (https://dev.twitter.com/docs/streaming-apis).\n",
    "Date range: March 24, 2012 to April 25, 2012.\n",
    "\n",
    "\n",
    "\n",
    "**** NETWORKS ****\n",
    "follower_gcc.anony.dat:\n",
    "    Format: anony.user1.ID anony.user2.ID\n",
    "    Anonymized reciprocal follower network.\n",
    "    Each edge is a pair of Twitter user who are following each other. After recovering the reciprocal follower network, the giant connected component is extracted.\n",
    "\n",
    "retweet_gcc.anony.dat:\n",
    "    Format: anony.user1.ID anony.user2.ID weight\n",
    "    Anonymized reciprocal retweet network.\n",
    "    Similarly to follower_gcc.anony.dat, but instead each edge is a pair of users who retweeted each other at least once during our observation time window. Weight is the sum of how many times user1 retweeted user2 or user2 retweeted user1.\n",
    "\n",
    "mention_gcc.anony.dat:\n",
    "    Format: anony.user1.ID anony.user2.ID weight\n",
    "    Anonymized reciprocal retweet network.\n",
    "    Similarly to follower_gcc.anony.dat, but instead each edge is a pair of users who mentioned each other at least once during our observation time window. Weight is the sum of how many times user1 mentioned user2 or user2 mentioned user1.\n",
    "\n",
    "\n",
    "\n",
    "**** HASHTAG SEQUENCES ****\n",
    "timeline_tag.anony.dat\n",
    "    Format: hashtag timestamp1,anony.user1.id timestamp2,anony.user2.id ...\n",
    "    Each line is a hashtag followed by the sequence of its adopters sorted by timestamp. A user is considered as an adopter of a hashtag once he/she starts using the hashtag. We only consider users who appear in the collected networks. The timestamp is the time when we see the hashtag in the user's tweets. The file includes both emergent hashtags and non-emergent ones.\n",
    "\n",
    "timeline_tag_rt.anony.dat\n",
    "    Format: hashtag timestamp1,anony.retweet_user1.id,anony.retweet_from_user1.id timestamp2,anony.retweet_user2.id,anony.tweet_from_user2.id ...\n",
    "    Each line is a hashtag followed by the sequence of its adopters retweeting about this hashtag from other users sorted by timestamp. A \"retweet_user\" retweets a message containing the hashtag from a \"retweet_from_user\". We only consider users who appear in the collected networks. The file includes both emergent hashtags and non-emergent ones.\n",
    "\n",
    "timeline_tag_men.anony.dat\n",
    "    Format: hashtag timestamp1,anony.mention_user1.id,anony.mentioned_user1.id timestamp2,anony.mention_user2.id,anony.mentioned_user2.id ...\n",
    "    Each line is a hashtag followed by the sequence of its adopters mentioning other users in messages containing this hashtags sorted by timestamp. A \"mention_user\" mentions a \"mentioned_user\" in a message with the target hashtag. We only consider users who appear in the collected networks. The file includes both emergent hashtags and non-emergent ones.\n",
    "\n",
    "\n",
    "\n",
    "** Note that users in these networks and timeline sequencies are anonymized in the same way so that the same IDs refer to the same Twitter users.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load from raw dat file into dataframes\n",
    "    #for relationships graphs\n",
    "df1 = pd.read_csv('../../../virality2013/follower_gcc.anony.dat',sep=' ',header=None,names=[\"user_1\",\"user_2\"])\n",
    "#df2 = pd.read_csv('../../../virality2013/mention_gcc.anony.dat',sep=' ',header=None,names=[\"user_1\",\"user_2\",\"weight\"])\n",
    "#df3 = pd.read_csv('../../../virality2013/retweet_gcc.anony.dat',sep=' ',header=None,names=[\"user_1\",\"user_2\",\"weight\"])\n",
    "    \n",
    "    #for hashtags spreading, the data format is too large to compute as matrix (over billion units)\n",
    "#df4 = pd.read_csv('../../../virality2013/timeline_tag.anony.dat',sep=' ',header=None,names=header_tl)\n",
    "#df5 = pd.read_csv('../../../virality2013/timeline_tag_men.anony.dat',sep=' ',header=None,names=header_tl_men)\n",
    "#df6 = pd.read_csv('../../../virality2013/timeline_tag_rt.anony.dat',sep=' ',header=None,names=header_tl_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Hashtags analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht=[]\n",
    "l=[]\n",
    "n=0\n",
    "with open('../../../virality2013/timeline_tag.anony.dat',\"r\") as f:\n",
    "    for line in f:\n",
    "        hashtag = line.split(\" \")[0]\n",
    "        size = len(line.split(\" \"))\n",
    "        ht.append(hashtag)\n",
    "        l.append(size)\n",
    "ht_df=pd.DataFrame()\n",
    "ht_df['hashtag']=ht\n",
    "ht_df['count_adopters']=l\n",
    "\n",
    "print(\"Max len for a row : %s\" %max(l))\n",
    "print(\"Nb of hashtags : %s\" %(len(ht)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_df = ht_df[ht_df['hashtag'].str.len() > 3] #filter meaningless hashtags\n",
    "ht_df.sort(columns='count_adopters',ascending=False,inplace=True)\n",
    "ht_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_df_ = ht_df[0:30]\n",
    "plt.figure(figsize=(12,12))\n",
    "a=sns.barplot(y=ht_df_['hashtag'],x=ht_df_['count_adopters'],palette=\"Blues_d\")\n",
    "a.set_title('Count for most popular hashtags')\n",
    "a.set_xlabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Light Network Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "g1 = nx.from_pandas_dataframe(df1,source=\"user_1\",target=\"user_2\") #.to_undirected()\n",
    "del df1 #supress to free memory\n",
    "g1 = g1.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "degree_sequence=sorted(nx.degree(g1).values(),reverse=True) # degree sequence\n",
    "#print \"Degree sequence\", degree_sequence\n",
    "dmax=max(degree_sequence)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.loglog(degree_sequence,'b-',marker='o')\n",
    "plt.title(\"Degree rank plot\")\n",
    "plt.ylabel(\"degree\")\n",
    "plt.xlabel(\"rank\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "degree_sequence=nx.degree(g1).values()\n",
    "#print \"Degree sequence\", degree_sequence\n",
    "dmax=max(degree_sequence)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.loglog(degree_sequence,'b-',marker='o')\n",
    "plt.title(\"Nodes degree plot\")\n",
    "plt.ylabel(\"degree\")\n",
    "plt.xlabel(\"node\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "degree_sequence=nx.degree(g1).values()\n",
    "#print \"Degree sequence\", degree_sequence\n",
    "dmax=max(degree_sequence)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(nx.degree(g1).keys(),degree_sequence)\n",
    "plt.title(\"Nodes degree plot\")\n",
    "plt.ylabel(\"degree\")\n",
    "plt.xlabel(\"node\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Random node sampling: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_nodes(node_list,ratio):\n",
    "    nb_sample = ratio*len(node_list)\n",
    "    nb_sample = int(nb_sample)\n",
    "    sample = np.random.choice(np.array(node_list),size=nb_sample)\n",
    "    return sample\n",
    "\n",
    "def get_nodes(g):\n",
    "    node_list = list(g.nodes())\n",
    "    return node_list\n",
    "\n",
    "def induced_graphed(vertex,g):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nodes1 = get_nodes(g1)\n",
    "sample = sample_nodes(nodes1,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Degree distribution sampling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extract counts and values\n",
    "count, value = deg       # automatically unpacks along first axis\n",
    "\n",
    "\n",
    "# Generate vector of probabilities for each node\n",
    "#p = [float(x)/sum(count) for x in count]\n",
    "p = count.astype(float) / count.sum()\n",
    "\n",
    "# Load into a random variable for sampling\n",
    "x = rv_discrete(values=(value,p))\n",
    "print x.rvs(size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Hashtags community:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_from_hash(hashtag,file_path,extended=True):\n",
    "    mat = []    \n",
    "    if extended == True:\n",
    "        with open(file_path,\"r\") as f:\n",
    "            for line in f:\n",
    "                if hashtag in line:\n",
    "                    mat.append(line.split(\" \"))\n",
    "        pading =    max([len(x) for x in mat])      \n",
    "        mat = np.array([np.array(pad(x,pading)) for x in mat])\n",
    "        return pd.DataFrame(mat)\n",
    "    else:\n",
    "        with open(file_path,\"r\") as f:\n",
    "            for line in f:\n",
    "                if hashtag == line.split(\" \")[0]:\n",
    "                    mat.append(line.split(\" \"))\n",
    "                    break\n",
    "        pading =    max([len(x) for x in mat])      \n",
    "        mat = np.array([np.array(pad(x,pading)) for x in mat])\n",
    "        return pd.DataFrame(mat)\n",
    "\n",
    "def pad(list_,length):\n",
    "    return list_[:length] + [np.nan]*(length-len(list_))\n",
    "\n",
    "def find_infected_vertex(df):\n",
    "    keep=[]\n",
    "    for col in df.columns[1:]:\n",
    "        array=split_clean(df[col])\n",
    "        keep.append([x[1] for x in array if isinstance(x,list)])\n",
    "    return list(set([val for sublist in keep for val in sublist]))\n",
    "\n",
    "def find_infected_timeline(df):\n",
    "    #return a dataframe with two columns, infected ids and timeline, by ascending order\n",
    "    timeline=[]\n",
    "    ids=[]\n",
    "    for col in df.columns[1:]:\n",
    "        array=split_clean(df[col])\n",
    "        timeline.append([x[0] for x in array if isinstance(x,list)])\n",
    "        ids.append([x[1] for x in array if isinstance(x,list)])\n",
    "    timeline=[val for sublist in timeline for val in sublist]\n",
    "    ids=[val for sublist in ids for val in sublist]\n",
    "    if len(ids)!=len(timeline):\n",
    "        print('Warning, timelines and ids do not match completly!')\n",
    "    inf_df = pd.DataFrame()\n",
    "    inf_df['infected_id']=ids\n",
    "    inf_df['timeline']=timeline\n",
    "    return inf_df\n",
    "\n",
    "def split_clean(list_):\n",
    "    array= [ x.split(\",\") for x in list_]\n",
    "    array= [ np.nan if x[0]=='nan' else [int(x[0]),int(x[1].strip('\\n'))] for x in array ]\n",
    "    return array\n",
    "\n",
    "def query_from_vertices(df,liste_):\n",
    "    df=df[(df['user_1'].isin(liste_)) | (df[\"user_2\"].isin(liste_))]\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_timestamp(timestamp):\n",
    "    return datetime.datetime.fromtimestamp(timestamp)#.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ht_df[ht_df[\"count_adopters\"]>=2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#guncontrol teamunicorns genocide  ThoughtsDuringSchool madonna\n",
    "hashtag='madonna'\n",
    "df = extract_from_hash(hashtag,\"../../../virality2013/timeline_tag.anony.dat\",extended = False)\n",
    "t=find_infected_vertex(df)\n",
    "t_timeline = find_infected_timeline(df)\n",
    "print(\"There are %s infected nodes for the hashtags %s\" %(len(t),hashtag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#automate function:\n",
    "def compute_timeline_sum(df,hashtag):\n",
    "    data = find_infected_timeline(df)\n",
    "    data=data.sort_values('timeline', ascending=True)\n",
    "    data['timeline'] = data.timeline.apply(lambda x : convert_timestamp(x))\n",
    "\n",
    "    #indicators\n",
    "    cumulated=range(0,len(data))\n",
    "    data[hashtag]=cumulated\n",
    "    data.set_index('timeline',inplace=True)\n",
    "    return data, data.index\n",
    "\n",
    "def compute_timeline_intensity(df,hashtag):\n",
    "    data = find_infected_timeline(df)\n",
    "    data=data.sort_values('timeline', ascending=True)\n",
    "    data['timeline'] = data.timeline.apply(lambda x : convert_timestamp(x))\n",
    "\n",
    "    #indicators\n",
    "    data.set_index('timeline',inplace=True)\n",
    "    #data.columns= [hashtag]\n",
    "    data[hashtag]=[1]*len(data)\n",
    "    return data, data.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "hashtags=['madonna','baseballswag','goodluck','yummy','ebay','yolo'] #'thoughtsduringschool','yolo'\n",
    "dfs=[]\n",
    "idx=[]\n",
    "for element in hashtags:\n",
    "    df = extract_from_hash(element,\"../../../virality2013/timeline_tag.anony.dat\",extended = False)\n",
    "    df,date_index = compute_timeline_sum(df,element)\n",
    "    dfs.append(df)\n",
    "    idx.append(date_index)\n",
    "\n",
    "date_start = min(map(lambda x : min(x), idx))\n",
    "date_end = max(map(lambda x : max(x), idx))\n",
    "date_range = pd.date_range(start=date_start,end=date_end, freq='15min')\n",
    "dataframe = pd.DataFrame(index=date_range)\n",
    "\n",
    "\n",
    "for element in zip(hashtags,dfs):\n",
    "    element[1][element[0]].plot(legend=True)\n",
    "\n",
    "a.set_xlabel(\"Date\")\n",
    "a.set_ylabel(\"Nombre d'infections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "hashtags=['madonna','baseballswag','goodluck','yummy','ebay','yolo'] #'thoughtsduringschool' \n",
    "dfs=[]\n",
    "idx=[]\n",
    "for element in hashtags:\n",
    "    df = extract_from_hash(element,\"../../../virality2013/timeline_tag.anony.dat\",extended = False)\n",
    "    df,date_index = compute_timeline_intensity(df,element)\n",
    "    dfs.append(df)\n",
    "    idx.append(date_index)\n",
    "\n",
    "date_start = min(map(lambda x : min(x), idx))\n",
    "date_end = max(map(lambda x : max(x), idx))\n",
    "#date_range = pd.date_range(start=date_start,end=date_end, freq='1H')\n",
    "date_range=list(set([val for sublist in idx for val in sublist]))\n",
    "dataframe = pd.DataFrame(index=date_range)\n",
    "\n",
    "\n",
    "for element in zip(hashtags,dfs):\n",
    "    df=element[1].groupby(element[1].index)[element[0]].sum()\n",
    "    df=df.resample('12H').sum().fillna(0)\n",
    "    df.plot(legend=True)\n",
    "    \n",
    "    #dataframe=df.resample('2H').sum()\n",
    "\n",
    "    \n",
    "\n",
    "a.set_xlabel(\"Date\")\n",
    "a.set_ylabel(\"Nombre d'infections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_shortest(couple):\n",
    "    if couple[0] not in master_nodes or couple[1] not in master_nodes:\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            nodes=nx.shortest_path(g1,source=couple[0],target=couple[1])\n",
    "        except Exception as e:\n",
    "            print('Failed at stage %s %s for reason : %s' %(couple[0],couple[1],e))\n",
    "            nodes=None\n",
    "        return nodes\n",
    "    \n",
    "def compute_shortest_speed(couple):\n",
    "    #without if control, loop is 10x faster\n",
    "    try:\n",
    "        nodes=nx.shortest_path(g1,source=couple[0],target=couple[1])\n",
    "    except Exception as e:\n",
    "        #print('Failed at stage %s %s for reason : %s' %(couple[0],couple[1],e))\n",
    "        nodes=None\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#combination multiprocessed\n",
    "#multiprocessing alternatives\n",
    "import multiprocessing as mp\n",
    "import datetime\n",
    "\n",
    "\n",
    "master_nodes=g1.nodes()\n",
    "combi = list(combinations(t,2))\n",
    "print(len(combi))\n",
    "\n",
    "try:\n",
    "    pool = mp.Pool(processes=8)\n",
    "    start_time=datetime.datetime.now()\n",
    "    nodes_ = pool.map(compute_shortest_speed,combi)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    elapsed=datetime.datetime.now()-start_time\n",
    "    print(\"Done processing in %s\" %(elapsed))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print('Pool successfuly closed!')\n",
    "\n",
    "\n",
    "#purge\n",
    "del combi\n",
    "del master_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter none\n",
    "nodes_ = [x  for x in nodes_ if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_flat(liste_):\n",
    "    flat=[]\n",
    "    for element in liste_:\n",
    "        try:\n",
    "            flat.append([x for x in element])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#nodes_=np.array(flat).flatten()\n",
    "flat=[val for sublist in nodes_ for val in sublist]\n",
    "print(len(flat))\n",
    "#flat_=[val for sublist in flat_ for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#flat = sample_nodes(flat,1)\n",
    "h=g1.subgraph(flat)\n",
    "len(h.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "color_map=[]\n",
    "size = []\n",
    "for node in h:\n",
    "    if node in t:\n",
    "        color_map.append('red')\n",
    "        size.append(40)\n",
    "    else:\n",
    "        color_map.append('grey')\n",
    "        size.append(10)\n",
    "        \n",
    "options_3 = {\n",
    " 'with_labels': False,\n",
    " 'node_color': color_map,\n",
    " 'node_size': size,\n",
    " 'linewidths': 1,\n",
    " 'width': 0.2,\n",
    " 'alpha': 0.4\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "nx.draw_spring(h, **options_3)\n",
    "plt.savefig(\"%s_shortest_path.png\" %(hashtag), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence=sorted(nx.degree(h).values(),reverse=True) # degree sequence\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.loglog(degree_sequence,'b-',marker='o')\n",
    "plt.title(\"Degree rank plot shortest path connectivity\")\n",
    "plt.ylabel(\"degree\")\n",
    "plt.xlabel(\"rank\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#add old\n",
    "df1 = pd.read_csv('../../../virality2013/follower_gcc.anony.dat',sep=' ',header=None,names=[\"user_1\",\"user_2\"])\n",
    "df1=query_from_vertices(df1,t)\n",
    "\n",
    "h_=nx.from_pandas_dataframe(df1,source=\"user_1\",target=\"user_2\") #.to_undirected()\n",
    "del df1 #supress to free memory\n",
    "h_ = h_.to_undirected()\n",
    "\n",
    "\n",
    "#compose\n",
    "composed = nx.compose(h,h_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence=sorted(nx.degree(h_).values(),reverse=True) # degree sequence\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.loglog(degree_sequence,'b-',marker='o')\n",
    "plt.title(\"Degree rank plot for grapped graph\")\n",
    "plt.ylabel(\"degree\")\n",
    "plt.xlabel(\"rank\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "color_map=[]\n",
    "size = []\n",
    "for node in composed:\n",
    "    if node in t:\n",
    "        color_map.append('red')\n",
    "        size.append(40)\n",
    "    else:\n",
    "        color_map.append('grey')\n",
    "        size.append(10)\n",
    "        \n",
    "options_3 = {\n",
    " 'with_labels': False,\n",
    " 'node_color': color_map,\n",
    " 'node_size': size,\n",
    " 'linewidths': 1,\n",
    " 'width': 0.2,\n",
    " 'alpha': 0.4\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "nx.draw_spring(h, **options_3)\n",
    "plt.savefig(\"output_pictures/%s_composed.png\" %(hashtag), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence=sorted(nx.degree(composed).values(),reverse=True) # degree sequence\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.loglog(degree_sequence,'b-',marker='o')\n",
    "plt.title(\"Degree rank plot for composed graph of both methodologies\")\n",
    "plt.ylabel(\"degree\")\n",
    "plt.xlabel(\"rank\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Extracting graph features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we create all the features used to assess the virality of a given graph in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Network features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of early adopters : is the set of distinct adopters in the earliest n tweets of a meme h.\n",
    "#For n=50\n",
    "def get_early_adopters(n, df):\n",
    "    user = []\n",
    "    for col in df.columns[1:(n+1)]:\n",
    "        array=split_clean(df[col])\n",
    "        user.append(array[0][1])\n",
    "    return user\n",
    "\n",
    "early_adopters = get_early_adopters(50, df)\n",
    "print(len(set(early_adopters)), \"early adopters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of first surface\n",
    "#The second surface includes uninfected users in the second surface of early adopters,\n",
    "#characterizing the number of potential adopters within two steps\n",
    "def get_surface(df,liste_):\n",
    "    df=df[(df['user_1'].isin(liste_)) | (df[\"user_2\"].isin(liste_))]\n",
    "    df=np.unique(df)\n",
    "    df = [x for x in df if x not in liste_] #on supprime les nodes infect√©s\n",
    "    return df\n",
    "first_surface = get_surface(df1, early_adopters)\n",
    "print(len(first_surface), \"size of first surface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Size of second surface\n",
    "#The first surface contains all the uninfected neighbors of early adopters of h.\n",
    "second_surface = get_surface(df1, first_surface)\n",
    "print(len(second_surface), \"size of first surface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Distance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Average step distance\n",
    "if nx.is_connected(h):\n",
    "    average_step_distance = nx.average_shortest_path_length(h)\n",
    "else:\n",
    "    average_step_distance=[]\n",
    "    for g in nx.connected_component_subgraphs(h):\n",
    "        average_step_distance.append(nx.average_shortest_path_length(g))\n",
    "average_step_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Community features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
