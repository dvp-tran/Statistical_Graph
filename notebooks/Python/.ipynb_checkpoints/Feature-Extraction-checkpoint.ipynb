{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from itertools import product\n",
    "from itertools import permutations\n",
    "import igraph as ig\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len for a row : 21811\n",
      "Nb of hashtags : 2899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:19: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#Extract Hashtags List\n",
    "ht=[]\n",
    "l=[]\n",
    "n=0\n",
    "with open('../../../virality2013/timeline_tag.anony.dat',\"r\") as f:\n",
    "    try:\n",
    "        for line in f:\n",
    "            hashtag = line.split(\" \")[0]\n",
    "            size = len(line.split(\" \"))\n",
    "            ht.append(hashtag)\n",
    "            l.append(size)\n",
    "    except:pass\n",
    "ht_df=pd.DataFrame()\n",
    "ht_df['hashtag']=ht\n",
    "ht_df['count_adopters']=l\n",
    "\n",
    "ht_df = ht_df[ht_df['hashtag'].str.len() > 3] #filter meaningless hashtags\n",
    "ht_df = ht_df[ht_df['count_adopters'] > 4] #filter meaningless hashtags\n",
    "ht_df.sort(columns='count_adopters',ascending=False,inplace=True)\n",
    "ht_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "print(\"Max len for a row : %s\" %max(l))\n",
    "print(\"Nb of hashtags : %s\" %(len(ht)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Fonctions et Préparation des data sets et du graph \n",
    "def extract_from_hash(hashtag,file_path,extended=True):\n",
    "    mat = []    \n",
    "    if extended == True:\n",
    "        with open(file_path,\"r\") as f:\n",
    "            try:\n",
    "                for line in f:\n",
    "                    if hashtag in line:\n",
    "                        mat.append(line.split(\" \"))\n",
    "            except:pass\n",
    "        pading =    max([len(x) for x in mat])      \n",
    "        mat = np.array([np.array(pad(x,pading)) for x in mat])\n",
    "        return pd.DataFrame(mat)\n",
    "    else:\n",
    "        with open(file_path,\"r\") as f:\n",
    "            try:\n",
    "                for line in f:\n",
    "                    if hashtag == line.split(\" \")[0]:\n",
    "                        mat.append(line.split(\" \"))\n",
    "                        break\n",
    "            except:pass\n",
    "        pading =    max([len(x) for x in mat])      \n",
    "        mat = np.array([np.array(pad(x,pading)) for x in mat])\n",
    "        return pd.DataFrame(mat)\n",
    "\n",
    "def pad(list_,length):\n",
    "    return list_[:length] + [np.nan]*(length-len(list_))\n",
    "\n",
    "def find_infected_vertex(df):\n",
    "    keep=[]\n",
    "    for col in df.columns[1:]:\n",
    "        array=split_clean(df[col])\n",
    "        keep.append([x[1] for x in array if isinstance(x,list)]) #Garde que les user_id\n",
    "    return list(set([val for sublist in keep for val in sublist]))\n",
    "\n",
    "def split_clean(list_):\n",
    "    array= [ x.split(\",\") for x in list_]\n",
    "    array= [ np.nan if x[0]=='nan' else [int(x[0]),int(x[1].strip('\\n'))] for x in array ]\n",
    "    return array\n",
    "\n",
    "def query_from_vertices(df,liste_):\n",
    "    df=df[(df['user_1'].isin(liste_)) | (df[\"user_2\"].isin(liste_))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Number of early adopters : is the set of distinct adopters in the earliest n tweets of a meme h.\n",
    "def get_early_adopters(n, df):\n",
    "    user = []\n",
    "    for col in df.columns[1:(n+1)]:\n",
    "        array=split_clean(df[col])\n",
    "        user.append(array[0][1])\n",
    "    return user\n",
    "\n",
    "#Size of first surface\n",
    "#The second surface includes uninfected users in the second surface of early adopters,\n",
    "#characterizing the number of potential adopters within two steps\n",
    "def get_surface(df,liste_):\n",
    "    df=df[(df['user_1'].isin(liste_)) | (df[\"user_2\"].isin(liste_))]\n",
    "    df=np.unique(df)\n",
    "    df = [x for x in df if x not in liste_] #on supprime les nodes infectés\n",
    "    return df\n",
    "\n",
    "#Average step distance\n",
    "def get_average_distance(h):\n",
    "    average_step_distance=0\n",
    "    if nx.is_connected(h):\n",
    "        try:\n",
    "            average_step_distance = nx.average_shortest_path_length(h)\n",
    "        except:pass\n",
    "    else:\n",
    "        average_step_distance=[]\n",
    "        for g in nx.connected_component_subgraphs(h):\n",
    "            try:\n",
    "                average_step_distance.append(nx.average_shortest_path_length(g))\n",
    "            except:pass\n",
    "        try:\n",
    "            average_step_distance = max(average_step_distance)\n",
    "        except:pass\n",
    "    return average_step_distance\n",
    "\n",
    "#Diameter\n",
    "#The diameter is the maximum distance between any two adopters of h within the first n tweets.\n",
    "def get_diameter(h):\n",
    "    diameter=0\n",
    "    if nx.is_connected(h):\n",
    "        try:\n",
    "            diameter = nx.diameter(h)\n",
    "        except:pass\n",
    "    else:\n",
    "        diameter=[]\n",
    "        for g in nx.connected_component_subgraphs(h):\n",
    "            diameter.append(nx.diameter(g))\n",
    "        try:\n",
    "            diameter = max(diameter)\n",
    "        except:pass\n",
    "    return diameter\n",
    "\n",
    "#Number of infected communities\n",
    "def get_infected_communities(h):\n",
    "    h_igraph = ig.Graph.Adjacency((nx.to_numpy_matrix(h) > 0).tolist())\n",
    "    communities = ig.Graph.community_infomap(h_igraph)\n",
    "    count = 0\n",
    "    for i in ig.clustering.VertexClustering.subgraphs(communities):\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "#Average step time duration\n",
    "def get_timestamps(df, n):\n",
    "    keep=[]\n",
    "    for col in df.columns[1:(n+1)]:\n",
    "        array=split_clean(df[col])\n",
    "        keep.append([x[0] for x in array if isinstance(x,list)]) #Garde que les timestamps\n",
    "    return list([val for sublist in keep for val in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction(h, df, df1):\n",
    "    early_adopters_list = get_early_adopters(n, df)\n",
    "    early_adopters = len(set((early_adopters_list)))\n",
    "    \n",
    "    first_surface_list = get_surface(df1, early_adopters_list)\n",
    "    first_surface = len(first_surface_list)\n",
    "    \n",
    "    second_surface = len(get_surface(df1, first_surface_list))\n",
    "    \n",
    "    average_distance = get_average_distance(h.subgraph(early_adopters_list))\n",
    "    \n",
    "    diameter = get_diameter(h.subgraph(early_adopters_list))\n",
    "    \n",
    "    infected_communities = get_infected_communities(h.subgraph(early_adopters_list))\n",
    "    \n",
    "    timestamps = get_timestamps(df, n)\n",
    "\n",
    "    average_step_time_duration = (timestamps[n-1] - timestamps[0]) / n-1\n",
    "    \n",
    "    root = np.sum((np.asarray(timestamps[1:n]) - np.asarray(timestamps[0:n-1]) - average_step_time_duration)**2)/(n-2)\n",
    "    cv_step_time_duration = 1/average_step_time_duration * np.sqrt(root)\n",
    "    \n",
    "    return early_adopters, first_surface, second_surface, average_distance, diameter, infected_communities, average_step_time_duration, cv_step_time_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_graph(hashtag):\n",
    "    \n",
    "    df = extract_from_hash(hashtag,\"virality2013/timeline_tag.anony.dat\",extended = False)\n",
    "    \n",
    "    if df[0:].shape[1] > 50:\n",
    "        t=find_infected_vertex(df)\n",
    "        df_temp = pd.read_csv('virality2013/follower_gcc.anony.dat',sep=' ',header=None,names=[\"user_1\",\"user_2\"])\n",
    "        df_temp=query_from_vertices(df_temp,t)\n",
    "\n",
    "        h=nx.from_pandas_dataframe(df_temp,source=\"user_1\",target=\"user_2\") #.to_undirected()\n",
    "        del df_temp\n",
    "\n",
    "        h = h.to_undirected()\n",
    "        \n",
    "        return h, df\n",
    "    else:\n",
    "        return df[0:].shape[1], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10factsaboutme  is done 538 more tweets to go\n",
      "10thingsifindattractive  is done 537 more tweets to go\n",
      "10thingsthatilike  is done 536 more tweets to go\n",
      "10thingsihate  is done 535 more tweets to go\n",
      "10peopleontwitteriwanttomeet  is done 534 more tweets to go\n",
      "1000aday  is done 533 more tweets to go\n",
      "10followersiwouldliketomeet  is done 532 more tweets to go\n",
      "10thingsiwanttohappen  is done 531 more tweets to go\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "n = 50\n",
    "\n",
    "\n",
    "dataset = []\n",
    "count=len(ht_df['hashtag'])\n",
    "df1 = pd.read_csv('virality2013/follower_gcc.anony.dat',sep=' ',header=None,names=[\"user_1\",\"user_2\"])\n",
    "for hashtag in ht_df['hashtag']:\n",
    "    count -= 1\n",
    "    h, df = get_graph(hashtag)\n",
    "    if type(h) !=int:\n",
    "        early_adopters, first_surface, second_surface, average_distance, diameter, infected_communities, average_step_time_duration, cv_step_time_duration = feature_extraction(h, df, df1)\n",
    "        dataset.append([hashtag, early_adopters, first_surface, second_surface, average_distance, diameter, infected_communities, average_step_time_duration, cv_step_time_duration])\n",
    "        print(hashtag, \" is done\", count, \"more tweets to go\")\n",
    "    else:\n",
    "        print(\"Not enough tweets, only\", h, \"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
