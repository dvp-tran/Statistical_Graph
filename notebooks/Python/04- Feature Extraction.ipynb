{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from itertools import product\n",
    "from itertools import permutations\n",
    "import igraph as ig\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len for a row : 363519\n",
      "Nb of hashtags : 1345913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:19: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#Extract Hashtags List\n",
    "ht=[]\n",
    "l=[]\n",
    "n=0\n",
    "with open('../../../virality2013/timeline_tag.anony.dat',\"r\") as f:\n",
    "    try:\n",
    "        for line in f:\n",
    "            hashtag = line.split(\" \")[0]\n",
    "            size = len(line.split(\" \"))\n",
    "            ht.append(hashtag)\n",
    "            l.append(size)\n",
    "    except:pass\n",
    "ht_df=pd.DataFrame()\n",
    "ht_df['hashtag']=ht\n",
    "ht_df['count_adopters']=l\n",
    "\n",
    "ht_df = ht_df[ht_df['hashtag'].str.len() > 3] #filter meaningless hashtags\n",
    "ht_df = ht_df[ht_df['count_adopters'] > 4] #filter meaningless hashtags\n",
    "ht_df.sort(columns='count_adopters',ascending=False,inplace=True)\n",
    "ht_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "print(\"Max len for a row : %s\" %max(l))\n",
    "print(\"Nb of hashtags : %s\" %(len(ht)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Fonctions et Préparation des data sets et du graph \n",
    "def extract_from_hash(hashtag,file_path,extended=True):\n",
    "    mat = []    \n",
    "    if extended == True:\n",
    "        with open(file_path,\"r\") as f:\n",
    "            try:\n",
    "                for line in f:\n",
    "                    if hashtag in line:\n",
    "                        mat.append(line.split(\" \"))\n",
    "            except:pass\n",
    "        pading =    max([len(x) for x in mat])      \n",
    "        mat = np.array([np.array(pad(x,pading)) for x in mat])\n",
    "        return pd.DataFrame(mat)\n",
    "    else:\n",
    "        with open(file_path,\"r\") as f:\n",
    "            try:\n",
    "                for line in f:\n",
    "                    if hashtag == line.split(\" \")[0]:\n",
    "                        mat.append(line.split(\" \"))\n",
    "                        break\n",
    "            except:pass\n",
    "        pading =    max([len(x) for x in mat])      \n",
    "        mat = np.array([np.array(pad(x,pading)) for x in mat])\n",
    "        return pd.DataFrame(mat)\n",
    "\n",
    "def pad(list_,length):\n",
    "    return list_[:length] + [np.nan]*(length-len(list_))\n",
    "\n",
    "def find_infected_vertex(df):\n",
    "    keep=[]\n",
    "    for col in df.columns[1:]:\n",
    "        array=split_clean(df[col])\n",
    "        keep.append([x[1] for x in array if isinstance(x,list)]) #Garde que les user_id\n",
    "    return list(set([val for sublist in keep for val in sublist]))\n",
    "\n",
    "def split_clean(list_):\n",
    "    array= [ x.split(\",\") for x in list_]\n",
    "    array= [ np.nan if x[0]=='nan' else [int(x[0]),int(x[1].strip('\\n'))] for x in array ]\n",
    "    return array\n",
    "\n",
    "def query_from_vertices(df,liste_):\n",
    "    df=df[(df['user_1'].isin(liste_)) | (df[\"user_2\"].isin(liste_))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Number of early adopters : is the set of distinct adopters in the earliest n tweets of a meme h.\n",
    "def get_early_adopters(n, df):\n",
    "    user = []\n",
    "    for col in df.columns[1:(n+1)]:\n",
    "        array=split_clean(df[col])\n",
    "        user.append(array[0][1])\n",
    "    return user\n",
    "\n",
    "#Size of first surface\n",
    "#The second surface includes uninfected users in the second surface of early adopters,\n",
    "#characterizing the number of potential adopters within two steps\n",
    "def get_surface(df,liste_):\n",
    "    df=df[(df['user_1'].isin(liste_)) | (df[\"user_2\"].isin(liste_))]\n",
    "    df=np.unique(df)\n",
    "    df = [x for x in df if x not in liste_] #on supprime les nodes infectés\n",
    "    return df\n",
    "\n",
    "#Average step distance\n",
    "def get_average_distance(h):\n",
    "    average_step_distance=0\n",
    "    if nx.is_connected(h):\n",
    "        try:\n",
    "            average_step_distance = nx.average_shortest_path_length(h)\n",
    "        except:pass\n",
    "    else:\n",
    "        average_step_distance=[]\n",
    "        for g in nx.connected_component_subgraphs(h):\n",
    "            try:\n",
    "                average_step_distance.append(nx.average_shortest_path_length(g))\n",
    "            except:pass\n",
    "        try:\n",
    "            average_step_distance = max(average_step_distance)\n",
    "        except:pass\n",
    "    return average_step_distance\n",
    "\n",
    "#Diameter\n",
    "#The diameter is the maximum distance between any two adopters of h within the first n tweets.\n",
    "def get_diameter(h):\n",
    "    diameter=0\n",
    "    if nx.is_connected(h):\n",
    "        try:\n",
    "            diameter = nx.diameter(h)\n",
    "        except:pass\n",
    "    else:\n",
    "        diameter=[]\n",
    "        for g in nx.connected_component_subgraphs(h):\n",
    "            diameter.append(nx.diameter(g))\n",
    "        try:\n",
    "            diameter = max(diameter)\n",
    "        except:pass\n",
    "    return diameter\n",
    "\n",
    "#Number of infected communities\n",
    "def get_infected_communities(h):\n",
    "    h_igraph = ig.Graph.Adjacency((nx.to_numpy_matrix(h) > 0).tolist())\n",
    "    communities = ig.Graph.community_infomap(h_igraph)\n",
    "    count = 0\n",
    "    for i in ig.clustering.VertexClustering.subgraphs(communities):\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "#Average step time duration\n",
    "def get_timestamps(df, n):\n",
    "    keep=[]\n",
    "    for col in df.columns[1:(n+1)]:\n",
    "        array=split_clean(df[col])\n",
    "        keep.append([x[0] for x in array if isinstance(x,list)]) #Garde que les timestamps\n",
    "    return list([val for sublist in keep for val in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction(h, df, df1):\n",
    "    early_adopters_list = get_early_adopters(n, df)\n",
    "    early_adopters = len(set((early_adopters_list)))\n",
    "    \n",
    "    first_surface_list = get_surface(df1, early_adopters_list)\n",
    "    first_surface = len(first_surface_list)\n",
    "    \n",
    "    second_surface = len(get_surface(df1, first_surface_list))\n",
    "    \n",
    "    average_distance = get_average_distance(h.subgraph(early_adopters_list))\n",
    "    \n",
    "    diameter = get_diameter(h.subgraph(early_adopters_list))\n",
    "    \n",
    "    infected_communities = get_infected_communities(h.subgraph(early_adopters_list))\n",
    "    \n",
    "    timestamps = get_timestamps(df, n)\n",
    "\n",
    "    average_step_time_duration = (timestamps[n-1] - timestamps[0]) / n-1\n",
    "    \n",
    "    root = np.sum((np.asarray(timestamps[1:n]) - np.asarray(timestamps[0:n-1]) - average_step_time_duration)**2)/(n-2)\n",
    "    cv_step_time_duration = 1/average_step_time_duration * np.sqrt(root)\n",
    "    \n",
    "    return early_adopters, first_surface, second_surface, average_distance, diameter, infected_communities, average_step_time_duration, cv_step_time_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_graph(hashtag):\n",
    "    \n",
    "    df = extract_from_hash(hashtag,\"../../../virality2013/timeline_tag.anony.dat\",extended = False)\n",
    "    \n",
    "    if df[0:].shape[1] > 50:\n",
    "        t=find_infected_vertex(df)\n",
    "        df_temp = pd.read_csv('../../../virality2013/follower_gcc.anony.dat',sep=' ',header=None,names=[\"user_1\",\"user_2\"])\n",
    "        df_temp=query_from_vertices(df_temp,t)\n",
    "\n",
    "        h=nx.from_pandas_dataframe(df_temp,source=\"user_1\",target=\"user_2\") #.to_undirected()\n",
    "        del df_temp\n",
    "\n",
    "        h = h.to_undirected()\n",
    "        \n",
    "        return h, df\n",
    "    else:\n",
    "        return df[0:].shape[1], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('oomf', ' is done', 180094, 'more tweets to go')\n",
      "('teamfollowback', ' is done', 180093, 'more tweets to go')\n",
      "('bahrain', ' is done', 180092, 'more tweets to go')\n",
      "('thoughtsduringschool', ' is done', 180091, 'more tweets to go')\n",
      "('yolo', ' is done', 180090, 'more tweets to go')\n",
      "('dearoomf', ' is done', 180089, 'more tweets to go')\n",
      "('retweet', ' is done', 180088, 'more tweets to go')\n",
      "('taurus', ' is done', 180087, 'more tweets to go')\n",
      "('followback', ' is done', 180086, 'more tweets to go')\n",
      "('winning', ' is done', 180085, 'more tweets to go')\n",
      "('wheniwaslittle', ' is done', 180084, 'more tweets to go')\n",
      "('capricorn', ' is done', 180083, 'more tweets to go')\n",
      "('gemini', ' is done', 180082, 'more tweets to go')\n",
      "('cancer', ' is done', 180081, 'more tweets to go')\n",
      "('shoutout', ' is done', 180080, 'more tweets to go')\n",
      "('nowplaying', ' is done', 180079, 'more tweets to go')\n",
      "('aries', ' is done', 180078, 'more tweets to go')\n",
      "('imagine', ' is done', 180077, 'more tweets to go')\n",
      "('pisces', ' is done', 180076, 'more tweets to go')\n",
      "('20thingsilove', ' is done', 180075, 'more tweets to go')\n",
      "('sagittarius', ' is done', 180074, 'more tweets to go')\n",
      "('virgo', ' is done', 180073, 'more tweets to go')\n",
      "('dailytweet', ' is done', 180072, 'more tweets to go')\n",
      "('boyfriend', ' is done', 180071, 'more tweets to go')\n",
      "('scorpio', ' is done', 180070, 'more tweets to go')\n",
      "('follow', ' is done', 180069, 'more tweets to go')\n",
      "('aquarius', ' is done', 180068, 'more tweets to go')\n",
      "('thingsigetalot', ' is done', 180067, 'more tweets to go')\n",
      "('fact', ' is done', 180066, 'more tweets to go')\n",
      "('libra', ' is done', 180065, 'more tweets to go')\n",
      "('10factsaboutme', ' is done', 180064, 'more tweets to go')\n",
      "('yougetmajorpointsif', ' is done', 180063, 'more tweets to go')\n",
      "('yougetpointsif', ' is done', 180062, 'more tweets to go')\n",
      "('10thingsifindattractive', ' is done', 180061, 'more tweets to go')\n",
      "('middleschoolmemories', ' is done', 180060, 'more tweets to go')\n",
      "('nowfollowing', ' is done', 180059, 'more tweets to go')\n",
      "('salute', ' is done', 180058, 'more tweets to go')\n",
      "('top100lies', ' is done', 180057, 'more tweets to go')\n",
      "('truthis', ' is done', 180056, 'more tweets to go')\n",
      "('tcot', ' is done', 180055, 'more tweets to go')\n",
      "('dead', ' is done', 180054, 'more tweets to go')\n",
      "('mythoughtsduringschool', ' is done', 180053, 'more tweets to go')\n",
      "('nationalbestfriendday', ' is done', 180052, 'more tweets to go')\n",
      "('theworstfeeling', ' is done', 180051, 'more tweets to go')\n",
      "('syria', ' is done', 180050, 'more tweets to go')\n",
      "('blessed', ' is done', 180049, 'more tweets to go')\n",
      "('aquarians', ' is done', 180048, 'more tweets to go')\n",
      "('icantstandwhen', ' is done', 180047, 'more tweets to go')\n",
      "('tweegram', ' is done', 180046, 'more tweets to go')\n",
      "('subtweet', ' is done', 180045, 'more tweets to go')\n",
      "('iadmit', ' is done', 180044, 'more tweets to go')\n",
      "('justsaying', ' is done', 180043, 'more tweets to go')\n",
      "('inourgeneration', ' is done', 180042, 'more tweets to go')\n",
      "('realtalk', ' is done', 180041, 'more tweets to go')\n",
      "('moviesthatnevergetold', ' is done', 180040, 'more tweets to go')\n",
      "('thingsisaywhilereadingmytl', ' is done', 180039, 'more tweets to go')\n",
      "('random', ' is done', 180038, 'more tweets to go')\n",
      "('mybiggestproblem', ' is done', 180037, 'more tweets to go')\n",
      "('love', ' is done', 180036, 'more tweets to go')\n",
      "('throwbackthursday', ' is done', 180035, 'more tweets to go')\n",
      "('teamiphone', ' is done', 180034, 'more tweets to go')\n",
      "('truth', ' is done', 180033, 'more tweets to go')\n",
      "('thingsmostpeoplelikebutidont', ' is done', 180032, 'more tweets to go')\n",
      "('oomfs', ' is done', 180031, 'more tweets to go')\n",
      "('imsinglebecause', ' is done', 180030, 'more tweets to go')\n",
      "('goodnight', ' is done', 180029, 'more tweets to go')\n",
      "('sorrynotsorry', ' is done', 180028, 'more tweets to go')\n",
      "('thatirritatesme', ' is done', 180027, 'more tweets to go')\n",
      "('trayvonmartin', ' is done', 180026, 'more tweets to go')\n",
      "('swag', ' is done', 180025, 'more tweets to go')\n",
      "('500aday', ' is done', 180024, 'more tweets to go')\n",
      "('10thingsthatilike', ' is done', 180023, 'more tweets to go')\n",
      "('free', ' is done', 180022, 'more tweets to go')\n",
      "('ifitwasuptome', ' is done', 180021, 'more tweets to go')\n",
      "('youknowwhatsannoying', ' is done', 180020, 'more tweets to go')\n",
      "('believe', ' is done', 180019, 'more tweets to go')\n",
      "('confessionhour', ' is done', 180018, 'more tweets to go')\n",
      "('10thingsihate', ' is done', 180017, 'more tweets to go')\n",
      "('gnation', ' is done', 180016, 'more tweets to go')\n",
      "('instagram', ' is done', 180015, 'more tweets to go')\n",
      "('realshit', ' is done', 180014, 'more tweets to go')\n",
      "('virgos', ' is done', 180013, 'more tweets to go')\n",
      "('wewontworkoutif', ' is done', 180012, 'more tweets to go')\n",
      "('twitterpeopleilove', ' is done', 180011, 'more tweets to go')\n",
      "('egypt', ' is done', 180010, 'more tweets to go')\n",
      "('thingsigottateachmyson', ' is done', 180009, 'more tweets to go')\n",
      "('imhappywhen', ' is done', 180008, 'more tweets to go')\n",
      "('followme', ' is done', 180007, 'more tweets to go')\n",
      "('aprilfools', ' is done', 180006, 'more tweets to go')\n",
      "('itscutewhen', ' is done', 180005, 'more tweets to go')\n",
      "('ifindthatattractive', ' is done', 180004, 'more tweets to go')\n",
      "('libras', ' is done', 180003, 'more tweets to go')\n",
      "('fail', ' is done', 180002, 'more tweets to go')\n",
      "('2omf', ' is done', 180001, 'more tweets to go')\n",
      "('weirdfactaboutme', ' is done', 180000, 'more tweets to go')\n",
      "('thevoiceuk', ' is done', 179999, 'more tweets to go')\n",
      "('ohwell', ' is done', 179998, 'more tweets to go')\n",
      "('thuglife', ' is done', 179997, 'more tweets to go')\n",
      "('instantfollowback', ' is done', 179996, 'more tweets to go')\n",
      "('1dfact', ' is done', 179995, 'more tweets to go')\n",
      "('mythoughtsduringsex', ' is done', 179994, 'more tweets to go')\n",
      "('14feb', ' is done', 179993, 'more tweets to go')\n",
      "('mustfollow', ' is done', 179992, 'more tweets to go')\n",
      "('cancers', ' is done', 179991, 'more tweets to go')\n",
      "('imsickof', ' is done', 179990, 'more tweets to go')\n",
      "('waystomakemehappy', ' is done', 179989, 'more tweets to go')\n",
      "('birthdaytweet', ' is done', 179988, 'more tweets to go')\n",
      "('justrememberthat', ' is done', 179987, 'more tweets to go')\n",
      "('10peopleontwitteriwanttomeet', ' is done', 179986, 'more tweets to go')\n",
      "('whatgetsmemad', ' is done', 179985, 'more tweets to go')\n",
      "('justsayin', ' is done', 179984, 'more tweets to go')\n",
      "('teamsingle', ' is done', 179983, 'more tweets to go')\n",
      "('sadtweet', ' is done', 179982, 'more tweets to go')\n",
      "('goodmorning', ' is done', 179981, 'more tweets to go')\n",
      "('fourwordsyoudontwanttohear', ' is done', 179980, 'more tweets to go')\n",
      "('scorpios', ' is done', 179979, 'more tweets to go')\n",
      "('whatmakesmesmile', ' is done', 179978, 'more tweets to go')\n",
      "('thingsthatilike', ' is done', 179977, 'more tweets to go')\n",
      "('4wordsyoudontwanttohear', ' is done', 179976, 'more tweets to go')\n",
      "('lolatgirlswho', ' is done', 179975, 'more tweets to go')\n",
      "('reasonswedonttalkanymore', ' is done', 179974, 'more tweets to go')\n",
      "('1omf', ' is done', 179973, 'more tweets to go')\n",
      "('teamlesbian', ' is done', 179972, 'more tweets to go')\n",
      "('foreveralone', ' is done', 179971, 'more tweets to go')\n",
      "('sheaintwifeymaterial', ' is done', 179970, 'more tweets to go')\n",
      "('followfriday', ' is done', 179969, 'more tweets to go')\n",
      "('iwanttopunchpeoplewho', ' is done', 179968, 'more tweets to go')\n",
      "('youknowwhatannoysme', ' is done', 179967, 'more tweets to go')\n",
      "('findsomeonewho', ' is done', 179966, 'more tweets to go')\n",
      "('bizitalk', ' is done', 179965, 'more tweets to go')\n",
      "('ididnttextyouback', ' is done', 179964, 'more tweets to go')\n",
      "('thingspeopledothatgetonmynerves', ' is done', 179963, 'more tweets to go')\n",
      "('stayawayfrommeif', ' is done', 179962, 'more tweets to go')\n",
      "('thatdepressingmoment', ' is done', 179961, 'more tweets to go')\n",
      "('damn', ' is done', 179960, 'more tweets to go')\n",
      "('icantdatesomeonethat', ' is done', 179959, 'more tweets to go')\n",
      "('teamheat', ' is done', 179958, 'more tweets to go')\n",
      "('1dfacts', ' is done', 179957, 'more tweets to go')\n",
      "('reasonsthatimsingle', ' is done', 179956, 'more tweets to go')\n",
      "('classic', ' is done', 179955, 'more tweets to go')\n",
      "('turnup', ' is done', 179954, 'more tweets to go')\n",
      "('soundcloud', ' is done', 179953, 'more tweets to go')\n",
      "('twothingsthatdontmix', ' is done', 179952, 'more tweets to go')\n",
      "('incollege', ' is done', 179951, 'more tweets to go')\n",
      "('howtopissafemaleoff', ' is done', 179950, 'more tweets to go')\n",
      "('ssnation', ' is done', 179949, 'more tweets to go')\n",
      "('throwback', ' is done', 179948, 'more tweets to go')\n",
      "('muchlove', ' is done', 179947, 'more tweets to go')\n",
      "('itstimeforyoutorealize', ' is done', 179946, 'more tweets to go')\n",
      "('turnon', ' is done', 179945, 'more tweets to go')\n",
      "('iknowthisonegirl', ' is done', 179944, 'more tweets to go')\n",
      "('icantlivewithout', ' is done', 179943, 'more tweets to go')\n",
      "('ritz2', ' is done', 179942, 'more tweets to go')\n",
      "('thingsthatfrustrateme', ' is done', 179941, 'more tweets to go')\n",
      "('twitterafterdark', ' is done', 179940, 'more tweets to go')\n",
      "('hotpeopleifollow', ' is done', 179939, 'more tweets to go')\n",
      "('thatawkwardmoment', ' is done', 179938, 'more tweets to go')\n",
      "('showerflow', ' is done', 179937, 'more tweets to go')\n",
      "('wordsyouwillneverhearmesay', ' is done', 179936, 'more tweets to go')\n",
      "('morefemalesshould', ' is done', 179935, 'more tweets to go')\n",
      "('respect', ' is done', 179934, 'more tweets to go')\n",
      "('nowthatimolder', ' is done', 179933, 'more tweets to go')\n",
      "('iwantsomebody', ' is done', 179932, 'more tweets to go')\n",
      "('teamautofollow', ' is done', 179931, 'more tweets to go')\n",
      "('bgc8', ' is done', 179930, 'more tweets to go')\n",
      "('thevoice', ' is done', 179929, 'more tweets to go')\n",
      "('thestruggle', ' is done', 179928, 'more tweets to go')\n",
      "('kuwait', ' is done', 179927, 'more tweets to go')\n",
      "('kindle', ' is done', 179926, 'more tweets to go')\n",
      "('teamnosleep', ' is done', 179925, 'more tweets to go')\n",
      "('mentionto', ' is done', 179924, 'more tweets to go')\n",
      "('bigtimerush', ' is done', 179923, 'more tweets to go')\n",
      "('ifollowback', ' is done', 179922, 'more tweets to go')\n",
      "('itscrazyhow', ' is done', 179921, 'more tweets to go')\n",
      "('favmusic', ' is done', 179920, 'more tweets to go')\n",
      "('music', ' is done', 179919, 'more tweets to go')\n",
      "('nomanshouldever', ' is done', 179918, 'more tweets to go')\n",
      "('1000aday', ' is done', 179917, 'more tweets to go')\n",
      "('real', ' is done', 179916, 'more tweets to go')\n",
      "('mybiggestfearis', ' is done', 179915, 'more tweets to go')\n",
      "('boss', ' is done', 179914, 'more tweets to go')\n",
      "('liestoldontwitter', ' is done', 179913, 'more tweets to go')\n",
      "('sometimesijustwant', ' is done', 179912, 'more tweets to go')\n",
      "('lmao', ' is done', 179911, 'more tweets to go')\n",
      "('obsessions', ' is done', 179910, 'more tweets to go')\n",
      "('fuck', ' is done', 179909, 'more tweets to go')\n",
      "('thingsblackpeopletakeseriously', ' is done', 179908, 'more tweets to go')\n",
      "('thingsilovetosee', ' is done', 179907, 'more tweets to go')\n",
      "('thatsthetruth', ' is done', 179906, 'more tweets to go')\n",
      "('amitheonlyonethat', ' is done', 179905, 'more tweets to go')\n",
      "('backinthedaywheniwasakid', ' is done', 179904, 'more tweets to go')\n",
      "('lowkey', ' is done', 179903, 'more tweets to go')\n",
      "('girlfacts', ' is done', 179902, 'more tweets to go')\n",
      "('truestory', ' is done', 179901, 'more tweets to go')\n",
      "('randomtweet', ' is done', 179900, 'more tweets to go')\n",
      "('hardestthingsinlife', ' is done', 179899, 'more tweets to go')\n",
      "('life', ' is done', 179898, 'more tweets to go')\n",
      "('photo', ' is done', 179897, 'more tweets to go')\n",
      "('arentyoutiredof', ' is done', 179896, 'more tweets to go')\n",
      "('fuckit', ' is done', 179895, 'more tweets to go')\n",
      "('itsfunnyhow', ' is done', 179894, 'more tweets to go')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-68ec7ec4bdc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhashtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mht_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hashtag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mearly_adopters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_surface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_surface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfected_communities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_step_time_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_step_time_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5742381c1b4a>\u001b[0m in \u001b[0;36mget_graph\u001b[0;34m(hashtag)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_infected_vertex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../../virality2013/follower_gcc.anony.dat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"user_2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdf_temp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_from_vertices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"user_1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"user_2\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.to_undirected()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3412d3061ffa>\u001b[0m in \u001b[0;36mquery_from_vertices\u001b[0;34m(df, liste_)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mquery_from_vertices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mliste_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliste_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliste_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36misin\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m   2488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m         \"\"\"\n\u001b[0;32m-> 2490\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/pandas/core/algorithms.pyc\u001b[0m in \u001b[0;36misin\u001b[0;34m(comps, values)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/pandas/core/algorithms.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# faster for larger cases to use np.in1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_np_version_under1p8\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismember_int64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/numpy/lib/arraysetops.pyc\u001b[0m in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;31m# Otherwise use sorting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrev_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mar2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/numpy/lib/arraysetops.pyc\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptional_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mergesort'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'quicksort'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "n = 50\n",
    "\n",
    "\n",
    "dataset = []\n",
    "count=len(ht_df['hashtag'])\n",
    "df1 = pd.read_csv('../../../virality2013/follower_gcc.anony.dat',sep=' ',header=None,names=[\"user_1\",\"user_2\"])\n",
    "for hashtag in ht_df['hashtag']:\n",
    "    count -= 1\n",
    "    h, df = get_graph(hashtag)\n",
    "    if type(h) !=int:\n",
    "        early_adopters, first_surface, second_surface, average_distance, diameter, infected_communities, average_step_time_duration, cv_step_time_duration = feature_extraction(h, df, df1)\n",
    "        dataset.append([hashtag, early_adopters, first_surface, second_surface, average_distance, diameter, infected_communities, average_step_time_duration, cv_step_time_duration])\n",
    "        print(hashtag, \" is done\", count, \"more tweets to go\")\n",
    "    else:\n",
    "        print(\"Not enough tweets, only\", h, \"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('features_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to load the data:\n",
    "\n",
    "import pickle\n",
    "with open('features_dataset.pkl', 'rb') as f:\n",
    "    new_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                oomf\n",
       "1                      teamfollowback\n",
       "2                             bahrain\n",
       "3                thoughtsduringschool\n",
       "4                                yolo\n",
       "5                            dearoomf\n",
       "6                             retweet\n",
       "7                              taurus\n",
       "8                          followback\n",
       "9                             winning\n",
       "10                     wheniwaslittle\n",
       "11                          capricorn\n",
       "12                             gemini\n",
       "13                             cancer\n",
       "14                           shoutout\n",
       "15                         nowplaying\n",
       "16                              aries\n",
       "17                            imagine\n",
       "18                             pisces\n",
       "19                      20thingsilove\n",
       "20                        sagittarius\n",
       "21                              virgo\n",
       "22                         dailytweet\n",
       "23                          boyfriend\n",
       "24                            scorpio\n",
       "25                             follow\n",
       "26                           aquarius\n",
       "27                     thingsigetalot\n",
       "28                               fact\n",
       "29                              libra\n",
       "                    ...              \n",
       "170                         mentionto\n",
       "171                       bigtimerush\n",
       "172                       ifollowback\n",
       "173                       itscrazyhow\n",
       "174                          favmusic\n",
       "175                             music\n",
       "176                   nomanshouldever\n",
       "177                          1000aday\n",
       "178                              real\n",
       "179                   mybiggestfearis\n",
       "180                              boss\n",
       "181                 liestoldontwitter\n",
       "182                sometimesijustwant\n",
       "183                              lmao\n",
       "184                        obsessions\n",
       "185                              fuck\n",
       "186    thingsblackpeopletakeseriously\n",
       "187                  thingsilovetosee\n",
       "188                     thatsthetruth\n",
       "189                 amitheonlyonethat\n",
       "190          backinthedaywheniwasakid\n",
       "191                            lowkey\n",
       "192                         girlfacts\n",
       "193                         truestory\n",
       "194                       randomtweet\n",
       "195               hardestthingsinlife\n",
       "196                              life\n",
       "197                             photo\n",
       "198                   arentyoutiredof\n",
       "199                            fuckit\n",
       "Name: hashtag, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht_df['hashtag'][0:200]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
